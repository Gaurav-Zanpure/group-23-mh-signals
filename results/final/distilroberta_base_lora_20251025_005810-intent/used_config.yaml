data:
  data_cfg: configs/data.yaml
logging:
  run_name: distilroberta_base_lora
  save_dir: results/runs
lora:
  lora_alpha: 128
  lora_dropout: 0.1
  r: 64
  target_modules:
  - roberta.encoder.layer.0.attention.self.query
  - roberta.encoder.layer.0.attention.self.key
  - roberta.encoder.layer.0.attention.self.value
  - roberta.encoder.layer.1.attention.self.query
  - roberta.encoder.layer.1.attention.self.key
  - roberta.encoder.layer.1.attention.self.value
  - roberta.encoder.layer.2.attention.self.query
  - roberta.encoder.layer.2.attention.self.key
  - roberta.encoder.layer.2.attention.self.value
  - roberta.encoder.layer.3.attention.self.query
  - roberta.encoder.layer.3.attention.self.key
  - roberta.encoder.layer.3.attention.self.value
  - roberta.encoder.layer.4.attention.self.query
  - roberta.encoder.layer.4.attention.self.key
  - roberta.encoder.layer.4.attention.self.value
  - roberta.encoder.layer.5.attention.self.query
  - roberta.encoder.layer.5.attention.self.key
  - roberta.encoder.layer.5.attention.self.value
model:
  max_length: 512
  name: distilroberta-base
  num_labels: 9
training:
  epochs: 10
  eval_batch_size: 32
  learning_rate: 1e-4
  output_dir: models/checkpoints/
  seed: 42
  threshold: 0.25
  train_batch_size: 8
  weight_decay: 0.01
